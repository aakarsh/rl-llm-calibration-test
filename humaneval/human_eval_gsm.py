# -*- coding: utf-8 -*-
"""human_eval_gsm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rHav1emQAMGfdMC6iEwMUq-uykuqo2-6
"""

import torch
from transformers import pipeline

from transformers import LlamaForCausalLM, LlamaTokenizer, AutoTokenizer
import torch

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM
)
import torch
from datasets import (
    load_dataset,
    Dataset
)
from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer
import numpy as np
import string
import json
import random
from random import shuffle

gm_dat = load_dataset("gsm8k","main")
gm8_sk = gm_dat['test']
openai_humaneval_dataset = load_dataset("openai_humaneval")
opennai_test = openai_humaneval_dataset["test"]
datasets_dict= {"human_eval": opennai_test,"gsm": gm8_sk}

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf",torch_dtype="auto", load_in_4bit=True)
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf",torch_dtype="auto",load_in_4bit=True)


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
def get_log_prob_of_completion(
        model,
        tokenizer,
        prompt,
        completion,
):
        """
        Convenience function for computing the log probability of a completion
        given a prompt. This is used to compute the log probability of the
        correct and incorrent labels given different trials for the different
        SuperGLUE tasks.
        """
        # tokenize the prompt and the completion
        # truncate so as to fit into to maximal context window of gpt-2
        # which is 1024 tokens
        input_ids = tokenizer(
                prompt + completion,
                return_tensors='pt',
                truncation=True,
                max_length=1024,
        )['input_ids'].to(device)

        # separately tokenize prompt
        # so as to access the logits for the completion only
        # when scoring the completion
        input_ids_prompt = tokenizer(
                prompt,
                return_tensors='pt',
                truncation=True,
                max_length=1024
        )['input_ids'].to(device)

        # create attention mask and position ids
        attention_mask = (input_ids != tokenizer.eos_token_id).to(dtype=torch.int64)
        position_ids = attention_mask.cumsum(-1)-1
        # get the logits for the completion
        with torch.no_grad():
                out = model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        position_ids=position_ids
                )

        # get the logits of the completion
        # for that, make a tensor of the logits
        # for the completion only
        # in particular, we shift the indices by one to the left to access logits of the
        # actual sequence tokens

        logits_completion = out.logits[:, :-1]
        logits_completion = logits_completion.squeeze()
        # get the log probabilities for the completion
        log_probs = torch.nn.functional.log_softmax(
                logits_completion,
                dim=-1
        )
        # retrieve the logit corresponding to the actual completion tokens
        try:
                log_completion_tokens = log_probs.gather(
                        dim=-1,
                        index=input_ids[:, 1:].squeeze().unsqueeze(-1)
                )
        except:
                log_completion_tokens = log_probs.gather(
                        dim=-1,
                        index=input_ids[:, 1:].unsqueeze(-1)
                )

        continuationConditionalLogProbs = log_completion_tokens[
                (input_ids_prompt.shape[-1]-1):
        ]
        completion_log_prob = torch.mean(
                continuationConditionalLogProbs
        ).cpu()
        return completion_log_prob





def format_questions(question,solutions):
  options = list(string.ascii_uppercase[0:len(solutions)])
  question_part = "Prompt:" + question
  answer_part = "Select the correct completion:\n"
  answer_rest = ""
  for letter, answer in zip(options,solutions):
    answer_rest += letter +'.' + answer + '\n'
  formatted = question_part + answer_part + answer_rest
  return formatted

def format_prompts_univ(dataset,nb_of_questions,idx ,none_option):
  options = list(string.ascii_uppercase[0:nb_of_questions+1])
  other_indexes = []
  numbers = list(range(0, len(opennai_test)))
  numbers.remove(idx)
  result = random.sample(numbers, nb_of_questions)
  options_list = [True] + ([False]*nb_of_questions)
  if dataset == "human_eval":
    humaneval_set = datasets_dict[dataset]
    prompt =  humaneval_set[idx]["prompt"]
    humaneval_set = datasets_dict["human_eval"]
    distractor_results = [humaneval_set[i]["canonical_solution"] for i in result]
    shuffling_options = [humaneval_set[idx]["canonical_solution"]]+ distractor_results
  elif dataset == "gsm":
    gsm_set = datasets_dict["gsm"]
    prompt =  gsm_set[idx]["question"]
    distractor_results = [gsm_set[i]["answer"] for i in result]
    shuffling_options = [gsm_set[idx]["answer"]]+ distractor_results
  c = list(zip(shuffling_options, options_list))
  random.shuffle(c)
  shuffling_options, list_options = zip(*c)
  formatted_prompt = format_questions(prompt,shuffling_options)
  answ_ind = options[list_options.index(True)]

  if none_option == True:
    none_letter = string.ascii_uppercase[nb_of_questions+1]
    options += string.ascii_uppercase[nb_of_questions+1]
    formatted_prompt += none_letter + ": None of the above"

  return formatted_prompt, answ_ind, options

example_prompt = format_prompts_univ("human_eval",2,10,True)

example_prompt = format_prompts_univ("human_eval",2,10,True)

results = []
for idx, other in enumerate(opennai_test):
  prompt_to_evaluate, answer_id, opt = format_prompts_univ("human_eval", 3, idx, True )
  options_probs = []
  for i in opt:
    option_prob = get_log_prob_of_completion(model = model, tokenizer = tokenizer, prompt = prompt_to_evaluate, completion = i )
    options_probs.append(option_prob.item())
  selection_results = dict(zip(opt, options_probs))
  chosen_selection = np.argmax(options_probs)
  print(idx)
  results.append({"model": "llama-7B-hf",
                  "dataset": "humaneval",
                  "context_results": selection_results ,
                  "chosen": opt[chosen_selection],
                  "answer": answer_id})
model_results = {"model":"llama-7B-hf","prompt-type":example_prompt,"results":results}

SAVE_DIR = "sample_data"

with open(SAVE_DIR+"/humaneval_7B_3options_none_final.json", "w") as outfile:
      json.dump(model_results, outfile, indent=4)