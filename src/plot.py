#%%
# Generated by Gemini
# Import seaborn
import seaborn as sns
import json
from functools import partial

# Load the test data
with open('../test/data/test_data.json') as f:
  test_data = json.load(f)

#%% 
def get_normalized_probabilities(model_results):
  completions = list(sorted(model_results[0]['context_results'].keys()))
  completion_probabilities = []
  truth_value = []
  for model_result in model_results:
    total = sum ([model_result['context_results'][completion] for completion in completions])
    completion_probabilities +=[model_result['context_results'][completion] / total for completion in completions]
    truth_value+= [(model_result['context_results'][completion] == model_result['answer']) and model_result['answer'] == model_result['chosen'] for completion in completions]
  return completion_probabilities, truth_value

model_results = test_data[0]['results']
completion_probabilities, truth_values = get_normalized_probabilities(model_results)
assert len(completion_probabilities) == len(truth_values)
#%%
# Apply the default theme
sns.set_theme()

import numpy as np
import matplotlib.pyplot as plt

def plot_calibration(prediction_probabilities, actual_labels,num_bins=50, range_start = 0 , range_end=.00001):
  # Sort predictions and corresponding actual labels
  sorted_indices = np.argsort(prediction_probabilities)
  sorted_probs = prediction_probabilities[sorted_indices]
  sorted_labels = actual_labels[sorted_indices]

  # Create equal-sized bins
  bin_edges = np.linspace(range_start, range_end, num_bins)# + 1)
  print("bin_edges", bin_edges)
  print("len:bin_edges", len(bin_edges))
  # Calculate frequency of correct predictions in each bin
  bin_counts = np.zeros(num_bins)
  bin_correct = np.zeros(num_bins)

  for i in range(len(sorted_probs)):
      bin_index = np.digitize(sorted_probs[i], bin_edges) - 1
      if len(bin_counts) < bin_index:
          print("bin_index", bin_index)
      bin_counts[bin_index] += 1
      bin_correct[bin_index] += sorted_labels[i]

  bin_accuracy = bin_correct / bin_counts # TODO Fix runtime issue.

  bin_means = (bin_edges[:-1] + bin_edges[1:]) / 2  # For plotting midpoints

  # Create the calibration chart
  plt.plot(bin_means, bin_means, '--', color='gray', label='Perfect Calibration')  # Diagonal line
  plt.plot(bin_means, bin_accuracy[:-1], 'o-', label='Model Calibration')
  plt.xlabel('Prediction Probability (P(True))')
  plt.ylabel('Frequency of Correct Predictions')
  plt.title('Calibration Chart')
  plt.legend()
  plt.show()

# Plot the calibration effect of the following

plot_calibration(np.array(completion_probabilities), 
                 np.array(truth_values), 
                 num_bins=10, range_start=0, range_end=1)
# %%
